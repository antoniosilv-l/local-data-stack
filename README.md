# Modern Data Stack

Ainda estou desenvolvendo este repositorio, qualquer coisa pode me chamar no LinkedIn.

Esse repositÃ³rio foi criado com o objetivo de estudar e construir uma arquitetura de dados moderna, seguindo os melhores padrÃµes.

## Arquitetura do Ambiente

![Arquitetura Modern Data Stack](docs/images/modern-data-stack.png)

### VisÃ£o Geral

A arquitetura implementada utiliza um conjunto de tecnologias modernas e open-source para criar um ambiente completo de dados, desde a ingestÃ£o atÃ© o consumo, com foco em governanÃ§a, qualidade e observabilidade.

## Stack TecnolÃ³gica

### ğŸ”„ **Data Ingestion**
- **Apache Flink**: Processamento de streaming em tempo real
- **Framework Personalizado**: Conectores customizados para fontes especÃ­ficas

### ğŸŒŠ **Ambiente de Dados**
- **Apache Airflow**: OrquestraÃ§Ã£o de workflows
- **Landing Zone**: Camada de dados brutos com Apache Iceberg
- **Data Vault**: Modelagem de dados historizada
- **Business Vault**: Regras de negÃ³cio aplicadas
- **Information Schema**: 
  - **OBT (One Big Table)**: Tabelas desnormalizadas
  - **Star Schema**: Modelagem dimensional

### âš™ï¸ **Data Processor**
- **dbt (Data Build Tool)**: TransformaÃ§Ãµes de dados em SQL

### ğŸ’¾ **Data Storage and SQL Engine**
- **MinIO**: Armazenamento de objetos S3-compatÃ­vel
- **Dremio**: Engine SQL distribuÃ­do e virtualizaÃ§Ã£o de dados

### ğŸ“Š **Data Governance and Monitoring**
- **Grafana**: Dashboards de monitoramento e observabilidade
- **Prometheus**: Coleta de mÃ©tricas e alertas
- **OpenLineage**: Rastreamento de linhagem de dados
- **OpenMetadata**: CatÃ¡logo de dados e descoberta
- **Great Expectations**: ValidaÃ§Ã£o e qualidade de dados

### ğŸ”§ **Services**
- **FastAPI**: APIs REST para exposiÃ§Ã£o de dados
- **Metabase**: Business Intelligence open-source
- **Looker**: Plataforma de BI empresarial
- **Tableau**: VisualizaÃ§Ã£o avanÃ§ada de dados

### ğŸ—ï¸ **Infraestrutura**
- **Docker**: ContainerizaÃ§Ã£o de aplicaÃ§Ãµes
- **GitHub**: Controle de versÃ£o e colaboraÃ§Ã£o
- **Git**: Versionamento de cÃ³digo
- **Jenkins**: Servidor de CI/CD para automaÃ§Ã£o de builds e deploys
- **Terraform**: Infrastructure as Code
- **Terragrunt**: Wrapper para Terraform
- **Pre-commit**: Hooks de validaÃ§Ã£o
- **SonarQube**: AnÃ¡lise de qualidade de cÃ³digo
- **Pytest**: Testes automatizados
- **Checkov**: Scanner de seguranÃ§a para Infrastructure as Code (Terraform, Docker, K8s)
- **Trivy**: Scanner de vulnerabilidades para containers e dependÃªncias
- **OpenTelemetry**: Observabilidade distribuÃ­da com traces, mÃ©tricas e logs

## Estrutura do RepositÃ³rio

```
data-stack/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ images/           # Diagramas da arquitetura
â”œâ”€â”€ airflow/              # DAGs do Airflow
â”œâ”€â”€ dbt/                  # TransformaÃ§Ãµes dbt
â”œâ”€â”€ infra/                # Infrastructure as Code
â”œâ”€â”€ docker/               # Containers e docker-compose
â”œâ”€â”€ tests/                # Testes automatizados
â”œâ”€â”€ scripts/              # Scripts de setup e utilitÃ¡rios
â””â”€â”€ README.md             # DocumentaÃ§Ã£o principal
```

## TO DO

### ğŸ“‹ **Fase 0: Infraestrutura Base**
- [ ] **Setup do Ambiente Local**
  - [ ] Criar mÃ³dulos Terraform
  - [ ] Setup do MinIO local
  - [ ] Configurar Dremio local
  - [ ] Setup bÃ¡sico do Airflow

- [ ] **Infrastructure as Code**
  - [ ] Criar mÃ³dulos Terraform para MinIO
  - [ ] Configurar Terragrunt para ambientes (dev/staging/prod)
  - [ ] Implementar Terraform para Flink cluster

- [ ] **CI/CD Pipeline**
  - [ ] Configurar Jenkins pipeline
  - [ ] Integrar Pre-commit hooks
  - [ ] Setup do SonarQube para anÃ¡lise de cÃ³digo
  - [ ] Configurar Checkov para validaÃ§Ã£o de IaC
  - [ ] Implementar Trivy para scan de vulnerabilidades

### ğŸ“‹ **Fase 1: Framework**
- [ ] **Construcao do Framework**
  - [ ] Organizacao do ambiente/pastas
  - [ ] Configuracao para extracoes JDBC
  - [ ] Configuracao para extracoes API
  - [ ] Configuracao para extracoes Arquivos
  - [ ] Configuracao para extracoes Streaming
  - [ ] Inclusao de testes
  - [ ] Inclusao de datasources

### ğŸ“Š **Fase 2: Data Ingestion & Processing**
- [ ] **Apache Flink**
  - [ ] Setup do cluster Flink
  - [ ] Desenvolver framework personalizado de conectores
  - [ ] Implementar jobs de streaming
  - [ ] Configurar checkpointing e recovery

- [ ] **Landing Zone (Iceberg)**
  - [ ] Configurar Apache Iceberg no MinIO
  - [ ] Implementar particionamento inteligente
  - [ ] Setup de compactaÃ§Ã£o automÃ¡tica
  - [ ] Configurar schema evolution

- [ ] **Apache Airflow**
  - [ ] Criar DAGs para orquestraÃ§Ã£o
  - [ ] Implementar sensores para dados
  - [ ] Configurar alertas e monitoramento
  - [ ] Setup de retry policies

### ğŸ”„ **Fase 3: Data Transformation**
- [ ] **dbt Implementation**
  - [ ] Setup do projeto dbt
  - [ ] Implementar Data Vault (Hubs, Links, Satellites)
  - [ ] Desenvolver Business Vault
  - [ ] Criar Information Schema (OBT + Star Schema)
  - [ ] Implementar testes de qualidade
  - [ ] Configurar documentaÃ§Ã£o automÃ¡tica

- [ ] **Great Expectations**
  - [ ] Setup do framework de qualidade
  - [ ] Criar expectativas para cada camada
  - [ ] Integrar com Airflow
  - [ ] Configurar alertas de qualidade

### ğŸ“ˆ **Fase 4: GovernanÃ§a & Monitoring**
- [ ] **OpenLineage**
  - [ ] Integrar com Airflow
  - [ ] Configurar tracking do dbt
  - [ ] Implementar lineage do Flink
  - [ ] Setup de visualizaÃ§Ã£o

- [ ] **OpenMetadata**
  - [ ] Setup do catÃ¡logo de dados
  - [ ] Configurar descoberta automÃ¡tica
  - [ ] Implementar classificaÃ§Ã£o de dados
  - [ ] Setup de polÃ­ticas de acesso

- [ ] **Observabilidade**
  - [ ] Implementar OpenTelemetry
  - [ ] Configurar Prometheus para mÃ©tricas
  - [ ] Setup do Grafana com dashboards
  - [ ] Configurar alertas inteligentes

### ğŸ¯ **Fase 5: Data Consumption**
- [ ] **APIs & Services**
  - [ ] Desenvolver APIs FastAPI
  - [ ] Integrar com Dremio
  - [ ] Implementar autenticaÃ§Ã£o/autorizaÃ§Ã£o
  - [ ] Setup de rate limiting

- [ ] **Business Intelligence**
  - [ ] Configurar Metabase
  - [ ] Criar dashboards padrÃ£o
  - [ ] Setup de self-service analytics

### ğŸ”’ **Fase 6: SeguranÃ§a & Compliance**
- [ ] **SeguranÃ§a**
  - [ ] Implementar encryption at rest/transit
  - [ ] Configurar RBAC no Dremio
  - [ ] Setup de auditoria
  - [ ] Implementar data masking

- [ ] **Compliance**
  - [ ] Implementar LGPD compliance
  - [ ] Setup de data retention policies
  - [ ] Configurar right to be forgotten
  - [ ] Documentar data lineage para compliance

### ğŸ§ª **Fase 7: Testes & Qualidade**
- [ ] **Testing Strategy**
  - [ ] Implementar unit tests (Pytest)
  - [ ] Criar integration tests
  - [ ] Setup de performance tests
  - [ ] Implementar data quality tests

- [ ] **AutomaÃ§Ã£o**
  - [ ] Configurar deploy automÃ¡tico
  - [ ] Setup de rollback automÃ¡tico
  - [ ] Implementar feature flags
  - [ ] Configurar blue-green deployment

### ğŸ“š **Fase 8: DocumentaÃ§Ã£o**
- [ ] **DocumentaÃ§Ã£o**
  - [ ] Documentar APIs
  - [ ] Documentar processos operacionais
  - [ ] Setup de knowledge base

- [ ] **CapacitaÃ§Ã£o**
  - [ ] Documentar best practices
  - [ ] Setup de sandbox environment

